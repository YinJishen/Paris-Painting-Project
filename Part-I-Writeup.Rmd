---
title: "Part-I-Writeup"
author: "Team-FP03"
date: "2019/12/7"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes: 
  \usepackage{float} 
  \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r packages}
suppressMessages(library(knitr))
suppressMessages(library(GGally))
suppressMessages(library(plyr))
suppressMessages(library(dplyr))
suppressMessages(library(mice))
```

```{r read-data}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
load("paintings_validation.RData")
```

# Introduction

In this project, we are going to explore what factors drove the price of paintings in 18th century Paris, and thus to identify possible overvalued and undervalued paintings.  

The dataset we are going to analyze is a series of auction transactions of paintings in Paris, ranging from 1764 to 1780. This dataset mainly contains the following information:  

1. Sale data, this include basic information about painters, dealers, end buyers, transaction dates and prices;  
2. Characteristics of paintings, such as their sizes, materials, number of figures and themes.  

To address our problem, we devide this project into two parts:  

1. In the first part, we carried out an exploratory data analysis. The target of this section is to understand the composition of our dataset and identify potential important variables.  
2. In the second part, a simple linear regression model was fit to the data, aiming to confirm important variables and interactions from the model selection process and to prepare for fitting a more complex model.  

# Exploratory Data Analysis  

In this section, we are going to explore our dataset in the following way: we first investigate the variables in the dataset to find their characteristics and possible relationships among each other; then we check the scatterplots between the response and each variable to identify potential important predictors.  

## Variable investigation

First of all, we can remove a few variables from the list of potential predictors simply based on their definitions:

Variable `price` is just the exponetial form of our target response `logprice`, and thus needs removing;  
Variable `count` is the same for all observations, therefore there's no point to use it in the model fitting.  

Besides these two, there exist quite a number of variables of interest:  

### Variables to impute

We've found that NA's exist in a lot of variables, and these NA's do not always indicate values missing completely at random. For example, from the R output below, we can see that `Surface` is not missing at random. Thus, instead of simply discarding observations containing NA's, we choose to impute the missing values with the observed ones.

For variables with a lot of blank values such as `endbuyer`, `type_intermed`, `material` and `mat`, we impute `n/a` into them to create a new category.  

```{r, echo = FALSE}
missing_Surface <- lm(paintings_train$logprice ~ is.na(paintings_train$Surface))
summary(missing_Surface)
```

```{r}
# replace blank space and NA of categorical variables with n/a
paintings_train_new <- paintings_train
paintings_train_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)] <- as.data.frame(sapply(paintings_train_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)], function(x) ifelse(x == "" | x == "-", "n/a", x)))
```

### Variables to manipulate

Variable `position` indicates the position of lot in the catalogue and is expressed as percentages. However, the maximum value of it in the dataset can be as large as `r round(max(paintings_train$position), 2)`, which are obviously typos. Similarly, there are observations with a series of size variables such as `Surface` all equal to $0$. As a result, observations with impossible `position` and `Surface` values are dropped.

Besides, `Shape` variable has some weird values, such as `oval` vs. `ovale`, and `ronde` vs. `round`, which are probably typos and thus need fixing.

Addtionally, if variables `origin_author` and `origin_cat` are known, the value of `diff_origin` is $100\%$ certain. Also, `type_intermed` incorporates all information of `Interm`. Thus, we decide to drop `diff_origin` and `Interm`.

In a similar manner, `Surface` should be known if `Diam_in`, or `Height_in` and `Width_in` are known at the same time. Also, note that `Surface` is the combination of `Surface_Rnd` and `Surface_Rect`. Thus, among all these variables mentioned, we keep just `Surface` in the model fitting process.

Variables `authorstandard`, `author`, `subject`, `sale`, `lot`, and `material` have way too many distinct values. Also, the possible values for these variables are too complicated and we decide not to use them in this simple model. When fitting a more complex model, it may be a good idea to convert them into new variables.

At last, in the dataset there exist strong correlations among some pairs of variables. For example, there is correlation between `Interm` & `type_intermed`, and `mat` & `materialCat`. In **Table 1** and **Table 2**, we display the contingency table for `Interm` vs. `type_intermed`, and as we can see, when `Interm` takes $0$ `type_intermed` always takes `n/a`; when `Interm` takes $1$, `type_intermed` takes other values. Thus, we decide to remove `Interm` and `materialCat`.  

```{r echo=FALSE}
# collinearity
kable(table(paintings_train_new$Interm, paintings_train_new$type_intermed), caption = "Interm vs type_intermed")
kable(table(paintings_train_new$mat, paintings_train_new$materialCat), caption = "mat vs materialCat")
```

## Important predictor identification

In this section we are going to evaluate scatter plots between our response `logprice` and each varaible after the manipulation from the previous part.  

```{r, include = FALSE}
# clean position
paintings_train_new <- paintings_train_new[-which(paintings_train_new$position > 1),]

# clean Shape
paintings_train_new[which(paintings_train_new$Shape == "ronde"), 28] <- "round"
paintings_train_new[which(paintings_train_new$Shape == "ovale"), 28] <- "oval"

# choose variables
paintings_train_new <- paintings_train_new %>% 
  select(-winningbidder, -authorstandard, -sale, -lot, -price, -count, -subject, -author, -Interm, -Surface_Rect, -Surface_Rnd, -material, -materialCat, -Height_in, -Width_in, -Diam_in)

# delete repeating data
paintings_train_new <- unique(paintings_train_new)

# imputate missing
set.seed(103)
imputed <- mice(paintings_train_new, m = 5)
paintings_train_new <- mice::complete(imputed)

# clean Surface
paintings_train_new <- paintings_train_new[-which(paintings_train_new$Surface == 0),]

# clean winningbiddertype
paintings_train_new$winningbiddertype <- case_when(
      paintings_train_new$winningbiddertype == "B" ~ "B",
      paintings_train_new$winningbiddertype == "BB" ~ "B",
      paintings_train_new$winningbiddertype == "BC" ~ "B",
      paintings_train_new$winningbiddertype == "C" ~ "C",
      paintings_train_new$winningbiddertype == "D" ~ "D",
      paintings_train_new$winningbiddertype == "DB" ~ "D",
      paintings_train_new$winningbiddertype == "DC" ~ "D",
      paintings_train_new$winningbiddertype == "DD" ~ "D",
      paintings_train_new$winningbiddertype == "E" ~ "E",
      paintings_train_new$winningbiddertype == "EB" ~ "E",
      paintings_train_new$winningbiddertype == "EBC" ~ "E",
      paintings_train_new$winningbiddertype == "EC" ~ "E",
      paintings_train_new$winningbiddertype == "ED" ~ "E",
      paintings_train_new$winningbiddertype == "U" ~ "n/a",
      paintings_train_new$winningbiddertype == "n/a" ~ "n/a"
    )
paintings_train_new$winningbiddertype <- as.factor(paintings_train_new$winningbiddertype)
```

```{r, fig.height = 24, fig.width = 16, fig.align = "center", fig.cap = "Plots of predictors versus logprice (1 to 23)", echo = FALSE}
# scatterplot
par(mfrow = c(6, 4))
for(i in c(1:7, 9:24)){
  plot(paintings_train_new[, i], paintings_train_new[, 8], xlab = colnames(paintings_train_new)[i], ylab = "logprice", cex.axis = 2, cex.lab = 2)
}
```

**Figure 1** above displays the scatter plots between `logprice` and the first $23$ variables in the dataset. Our target is to identify variables that show a strong relationship with the response. Bearing this in mind, it is easy to notice that variables `dealer`, `year`, `origin_author`, `winningbiddertype`, `endbuyer`, `type_intermed` and `finished` appear to have the strongest relationship with `logprice`. Also, there seem to be a very weak relationship between `position` and `logprice` as well. In addition, variables such as `Surface` are clustered near the beginning of x axis, and thus we decide to apply log transformations on them and have a closer look afterwards.

```{r, fig.height = 20, fig.width = 16, fig.align = "center", fig.cap = "Plots of predictors versus logprice (24 to 42)", echo = FALSE}
# scatterplot
par(mfrow = c(5, 4))
for(i in 25:43){
  plot(paintings_train_new[, i], paintings_train_new[, 8], xlab = colnames(paintings_train_new)[i], ylab = "logprice", cex.axis = 2, cex.lab = 2)
}
```

$~$

**Figure 2** above display the scatter plots between `logprice` and the rest of the variables in the dataset. As we can see, most of the binary categorical variables fail to present a strong relationship with the response. The only exception is `lrgfont`, which corresponds to quite different response values at the two different levels.  

For `Surface`, we can do log transformation to the corresponding predictors to see their relationship with `logprice` at a greater detail in **Figure 3**.  

```{r, fig.height = 4, fig.width = 4, fig.align = "center", fig.cap = "Plots of log Surface versus logprice"}
plot(log(paintings_train_new[, 15]), paintings_train_new[, 8], xlab = paste("log(", colnames(paintings_train_new)[15], ")", sep = ""), ylab = "logprice")
```

As we can see from **Figure 3**, there seem to be a weak relationship between `logprice` and log-transformed `Surface`. Intuitively, the surface of paintings should indeed be correlated to their prices.  

```{r fig.cap = "Variable Importance Measures in Random Forests"}
# log transformation to Surface
paintings_train_new$Surface <- log(paintings_train_new$Surface)
colnames(paintings_train_new)[15] <- "log_Surface"
```

Also, notice that `winningbiddertype` has too many levels, which may result in difficulties both in model fitting and in interpretation. Thus, we decide to apply the following transformation on `winningbiddertype`:

Observations with levels `B,BB,BC` are combined to have level `B`;  
Observations with levels `C` remains untouched;  
Observations with levels `D,DB,DC,DD` are combined to have level `D`;  
Observations with levels `E,EB,EBC,EC,ED` are combined to have level `E`;  
Blank space and unknown observations are combined to have level `n/a`.

The rationale for the above transformation is that, the bidder who actually attended the auction had the most important influence on the sale price.

In conclusion, after our manipulation of the dataset and inspection of the relationships between response and each variable, we reckon that variables `position`, `dealer`, `year`, `origin_author`, `winningbiddertype`, `endbuyer`, `type_intermed`, `finished`, `lrgfont` and the log transformation of `Surface` are the most important variables in terms of scatter plots and their definitions. However, we need formal model fitting and selection process to decide the variables and interactions to use.

# Model fitting

In this section, we are going to present the development and assessment of our simple model.

```{r cache = TRUE}
# full model
model_test <- lm(logprice ~ (dealer + year + origin_author + endbuyer + log_Surface + finished + lrgfont + position + winningbiddertype + type_intermed)^2, data = paintings_train_new)

# BIC
model1 <- step(model_test, k = log(nobs(model_test)), direction = "both", trace = F)
```

Above all, we display the summary and anova table for our final model

```{r}
summary(model1)
anova(model1)
```

The following is the process of building model:

1. for our initial model, we decide to incorporate all the important predictors in EDA.

2. we put the chosen main predictors and all their interactions into a full model. Then we use BIC to choose important predictors and interactions for us.

3. in the simply model, we have $8$ main predictors and $1$ interactions. Roughly $61\%$ variation of dependent variables are explained by this model. By looking at the ANOVA table of the model, all of the variables are significant at the $5\%$ level, which indicates that the variables in the model are reasonable.

```{r, warning = FALSE, fig.height = 8, fig.width = 8, fig.align = "center", fig.cap = "Diagnostic Plots"}
par(mfrow = c(2, 2))
plot(model1)
```

In the Residual vs Fitted plot, the zero level horizontal line is nearly flat and almost all points are randomly distributed around $0$, indicating no violation for the linearity assumption.

In the Normal Q-Q plot, a slight light tail exists, but it's not a serious problem. Most of the residuals do seem to follow a normal distribution. 

In the Scale-Location plot, the red line suggests that there is a small pattern for the resuduals. The residuals will increase first and then decrease a little bit. However, the points are very well randomly distributed around the zero level horizontal line, so the violation of constant variance is not significant enough to be very concerning.

In the Residuals vs Leverage plot, there are neither actually influential nor potentially influential points.

Generally, the diagnostic plots tell us that the linear model we get fit the training data very well and no model assumptions are violated.

```{r warning=FALSE}
coef <- summary(model1)$coefficients
kable(data.frame(Estimate = coef[,1],
                 CI_Low = coef[,1] - 1.96*coef[, 2],
                 CI_Up = coef[,1] + 1.96*coef[, 2]),
      digits = 4,
      caption = "Summary of coefficients and confidence intervals")
```

In **Table 3** above, we can see that part of the variables have high estimated coefficients compared to others. It may indicate the importance of the variables or that there are some potential problems in the linear model. There also exist several variables whose condifence intevals contain $0$. These variables may either have positive or negative effects on the price, which means the model is still not very satisfactory. we will use a more complicated model in the next part to improve the performance of the model.

# Summary and Conclusions

In our final model, the baseline price is $e^{-157}$ livres, which is approximately $0$ livres. It represents the price of a painting under baseline categories for all categorical variables, such as `dealer`, `origin_author` and `endbuyer`,etc.

According to the coefficients table above, predictors `year` and `winningbiddertype` have huge impact on the price sale. For `year`, althouth its coefficient is not large compared to others, the big numeric value itself will have impact on the price. Besides `year`, for the dummy variables, `winningbiddertype` is another important predictor that affects the price most.

Thus, the two most important variables are `year` and `winningbiddertype`. And the only interaction we have is the one between `year` and `winningbiddertype`. So it's natural to say that the interaction is also improtant.

Our model also has limitations. We choose all the main predictors from EDA, so we may actually ignore some relatively important predictors that are not identified through EDA. In our simple model, predictors `year`, `winningbiddertype`, and `year:winningbiddertype` look a little bit overly important compared to all other variables. It's questionalble for such a large data set. Besides, we only use the linear model to fit the data, resulting in a few large coefficents and standard deviations. Furthermore, the big estimated coefficients make us hard to interpret the model to the art historian. Thus, we may use nonlinear model to shrink the coefficients in the next part. There may even exist some more complicated relationships in the data such as polynomial, which still needs to be explored.

In our model, for every one year after the previous year, we expect that the price of the painting will be $e^{0.09}$ times higher, and we are $95\%$ confident that the fluction is between $e^{-0.08}$ to $e^{0.26}$, which is from $0.92$ to $1.30$.

Given all other conditions unchanged (eg: same dealer, same year, same origin, etc.), we expect the price of the painting will be $e^{-110}$ times higher if the type of winning bidder is a collector. And we are $95\%$ confident that the price fluction will be between $e^{-410}$ and $e^{191}$ times higher.

Given all other conditions unchanged (eg: same dealer, same year,same origin,etc.), we expect the price of painting will be $e^{10}$ times higher if the type of winning bidder is a dealer. And we are $95\%$ confident that the price fluction will be between $e^{-286}$ and $e^{307}$ times higher. 

Given all other conditions unchanged (eg: same dealer, same year,same origin,etc.), we expect the price of painting will be $e^{-210}$ times higher if the type of winning bidder is an expert organizing the sale. And we are $95\%$ confident that the price fluction will be between $e^{-511}$ and $e^{91}$ times higher. 

So we suggest the art historians that the painting bid by dealer with a larger year will have a high value. 

```{r predict-model1, echo = FALSE, warning = FALSE, include = FALSE}
# test data
paintings_test_new <- paintings_test

# replace blank space and NA of categorical variables with n/a
paintings_test_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)] <- as.data.frame(
  sapply(paintings_test_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)],
         function(x) ifelse(paste(x) == "" | paste(x) == "-",
                            "n/a", paste(x))))

# clean position
paintings_test_new[which(paintings_test_new$position > 1), 3] <-
  paintings_test_new[which(paintings_test_new$position > 1), 3]/100

# clean Shape
paintings_test_new[which(paintings_test_new$Shape == "ronde"), 28] <-
  "round"
paintings_test_new[which(paintings_test_new$Shape == "ovale"), 28] <-
  "oval"

# choose variables
paintings_test_new <- paintings_test_new %>% 
  select(-winningbidder, -authorstandard, -sale, -lot, -price, -count,
         -subject, -author, -Interm, -Height_in, -Width_in, -Diam_in,
         -Surface_Rect, -Surface_Rnd, -material, -materialCat)

# clean Surface
paintings_test_new[which(paintings_test_new$Surface == 0), 15] <- NA

# imputate missing
set.seed(103)
imputed_test <- mice(paintings_test_new, m = 5)
paintings_test_new <- mice::complete(imputed_test)

# log transformation to Surface
paintings_test_new$Surface <- log(paintings_test_new$Surface)
colnames(paintings_test_new)[15] <- "log_Surface"

# clean winningbiddertype
paintings_test_new$winningbiddertype <- case_when(
      paintings_test_new$winningbiddertype == "B" ~ "B",
      paintings_test_new$winningbiddertype == "BB" ~ "B",
      paintings_test_new$winningbiddertype == "BC" ~ "B",
      paintings_test_new$winningbiddertype == "C" ~ "C",
      paintings_test_new$winningbiddertype == "D" ~ "D",
      paintings_test_new$winningbiddertype == "DB" ~ "D",
      paintings_test_new$winningbiddertype == "DC" ~ "D",
      paintings_test_new$winningbiddertype == "DD" ~ "D",
      paintings_test_new$winningbiddertype == "E" ~ "E",
      paintings_test_new$winningbiddertype == "EB" ~ "E",
      paintings_test_new$winningbiddertype == "EBC" ~ "E",
      paintings_test_new$winningbiddertype == "EC" ~ "E",
      paintings_test_new$winningbiddertype == "ED" ~ "E",
      paintings_test_new$winningbiddertype == "U" ~ "n/a",
      paintings_test_new$winningbiddertype == "n/a" ~ "n/a"
    )
paintings_test_new$winningbiddertype <- as.factor(paintings_test_new$winningbiddertype)
```

```{r predict-model-final, echo=FALSE, include=FALSE}
# change model1 or update as needed
predictions <- as.data.frame(
  exp(predict(model1, newdata = paintings_test_new, 
              interval = "pred")))
save(predictions, file="predict-test.Rdata")
```

```{r predict-model2, echo = FALSE, warning = FALSE, include = FALSE}
# test data
paintings_val_new <- paintings_validation

# replace blank space and NA of categorical variables with n/a
paintings_val_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)] <- as.data.frame(
  sapply(paintings_val_new[, c(4, 6:9, 12:22, 25, 27:28, 30:32, 34:59)],
         function(x) ifelse(paste(x) == "" | paste(x) == "-",
                            "n/a", paste(x))))

# clean position
paintings_val_new[which(paintings_val_new$position > 1), 3] <-
  paintings_val_new[which(paintings_val_new$position > 1), 3]/100

# clean Shape
paintings_val_new[which(paintings_val_new$Shape == "ronde"), 28] <-
  "round"
paintings_val_new[which(paintings_val_new$Shape == "ovale"), 28] <-
  "oval"

# choose variables
paintings_val_new <- paintings_val_new %>% 
  select(-winningbidder, -authorstandard, -sale, -lot, -price, -count,
         -subject, -author, -Interm, -Height_in, -Width_in, -Diam_in,
         -Surface_Rect, -Surface_Rnd, -material, -materialCat)

# clean Surface
paintings_val_new[which(paintings_val_new$Surface == 0), 15] <- NA

# imputate missing
set.seed(103)
imputed_val <- mice(paintings_val_new, m = 5)
paintings_val_new <- mice::complete(imputed_val)

# log transformation to Surface
paintings_val_new$Surface <- log(paintings_val_new$Surface)
colnames(paintings_val_new)[15] <- "log_Surface"

# clean winningbiddertype
paintings_val_new$winningbiddertype <- case_when(
      paintings_val_new$winningbiddertype == "B" ~ "B",
      paintings_val_new$winningbiddertype == "BB" ~ "B",
      paintings_val_new$winningbiddertype == "BC" ~ "B",
      paintings_val_new$winningbiddertype == "C" ~ "C",
      paintings_val_new$winningbiddertype == "D" ~ "D",
      paintings_val_new$winningbiddertype == "DB" ~ "D",
      paintings_val_new$winningbiddertype == "DC" ~ "D",
      paintings_val_new$winningbiddertype == "DD" ~ "D",
      paintings_val_new$winningbiddertype == "E" ~ "E",
      paintings_val_new$winningbiddertype == "EB" ~ "E",
      paintings_val_new$winningbiddertype == "EBC" ~ "E",
      paintings_val_new$winningbiddertype == "EC" ~ "E",
      paintings_val_new$winningbiddertype == "ED" ~ "E",
      paintings_val_new$winningbiddertype == "U" ~ "n/a",
      paintings_val_new$winningbiddertype == "n/a" ~ "n/a"
    )
paintings_val_new$winningbiddertype <- as.factor(paintings_val_new$winningbiddertype)
```

```{r predict-model-validation, echo=FALSE, include=FALSE}
# change model1 or update as needed
predictions <- as.data.frame(
  exp(predict(model1, newdata = paintings_val_new, 
              interval = "pred")))
save(predictions, file="predict-validation.Rdata")
```
